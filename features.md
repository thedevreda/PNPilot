# Project Architecture: PNpilot Scalable MVP

## ğŸ¯ Goal:
A hosted scraping-based MVP that allows users to upload part numbers, scrape multiple vendor websites for prices, and return the cheapest offer, all securely and efficiently.

---

## ğŸ§± Core Tech Stack (With Prioritization & Focus Areas)

### 1. ğŸ§  Python (Main Language) â€” **FOCUS MOST**
- Web scraping: `requests`, `BeautifulSoup`, `lxml`
- Automation: `time`, `random`, `schedule`, `os`
- File handling: `pandas`, `csv`, `dotenv`

âœ… **Focus on:**
- Writing reusable scraping functions
- Handling exceptions, CAPTCHA logic
- Login/session handling with cookies and tokens


### 2. ğŸŒ Flask or FastAPI â€” **VERY HIGH PRIORITY**
- Converts your bot to a web app/backend
- Exposes API endpoints for uploads, scraping, and downloads

âœ… **Focus on:**
- Building secure file upload/download endpoints
- User session management
- API design (POST /scrape, GET /status, etc)


### 3. ğŸ“¦ Streamlit â€” **Important for UI MVP**
- Frontend UI for uploading part number lists and viewing results

âœ… **Focus on:**
- File uploader (`st.file_uploader()`)
- Data table display and alert banners
- Button-triggered backend interaction


### 4. ğŸ§° Celery + Redis â€” **Essential for Scaling Later**
- Manages background tasks (scraping jobs)

âœ… **Focus on:**
- Creating asynchronous jobs with retry logic
- Tracking task states (PENDING, SUCCESS, FAILED)
- Integrating Flask + Celery with Redis broker


### 5. ğŸ“Š PostgreSQL + SQLAlchemy â€” **Important for Storage**
- Stores users, job statuses, and results

âœ… **Focus on:**
- Creating tables for jobs, users, results
- Writing ORM models and queries
- Using SQLAlchemy to link data to Flask


### 6. ğŸ³ Docker â€” **Deployment Critical**
- Containerize app for consistent deployment

âœ… **Focus on:**
- Writing `Dockerfile` and `docker-compose.yml`
- Separating services: Flask, Celery, Redis, PostgreSQL


### 7. â˜ï¸ Cloud Hosting â€” **Deploy MVP**
- Use Render, Railway, or Heroku for simplicity

âœ… **Focus on:**
- Deploying multi-service apps (Flask + worker + DB)
- Auto redeployment on code push
- Setting env variables in cloud dashboard


### 8. ğŸ” Proxy & CAPTCHA Avoidance â€” **Optional but Useful**
- Rotating proxies or tools like ScraperAPI

âœ… **Focus on:**
- Handling request retries
- Implementing user-agent rotation
- Integrating proxy list fallback


### 9. ğŸ” Authentication â€” **User Access Control**
- Prevent abuse and allow user-specific data

âœ… **Focus on:**
- Basic login/signup (Flask-Login or OAuth)
- Secure credential storage with hashed passwords
- Optional: JWT tokens


### 10. ğŸ“ˆ Monitoring & Logging
- Track performance, failures, and system status

âœ… **Focus on:**
- Use `logging`, `sentry.io`, or `Flask-Logging`
- Show alerts in UI on failures (e.g. part not found)

---

## ğŸ“ Suggested Folder Structure

```
pnpilot/
â”œâ”€â”€ app.py                  # Flask app
â”œâ”€â”€ streamlit_app.py        # UI frontend
â”œâ”€â”€ bot/
â”‚   â”œâ”€â”€ scraper.py          # Scraping logic
â”‚   â”œâ”€â”€ login.py            # Login/session functions
â”‚   â”œâ”€â”€ parsers.py          # Site-specific HTML parsers
â”‚   â”œâ”€â”€ tasks.py            # Celery async tasks
â”œâ”€â”€ models/
â”‚   â””â”€â”€ database.py         # SQLAlchemy models
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input_parts.csv     # Uploaded files
â”‚   â”œâ”€â”€ processed.txt       # Track completed parts
â”‚   â”œâ”€â”€ failed.txt          # Log failed searches
â”œâ”€â”€ .env                    # API keys and credentials
â”œâ”€â”€ Dockerfile              # Docker config
â”œâ”€â”€ docker-compose.yml      # Multi-service app config
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âœ… Recommendation
You already have a **strong base** with Python, Pandas, SQL. Now focus on:
1. **Flask** â€” run your scraper as an API.
2. **Streamlit** â€” give it a clean interface.
3. **Celery + Redis** â€” handle multiple jobs at once.
4. **Docker** â€” deploy it all cleanly to a cloud host.

